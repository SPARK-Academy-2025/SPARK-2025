{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2542390,"sourceType":"datasetVersion","datasetId":1541666},{"sourceId":12487235,"sourceType":"datasetVersion","datasetId":7879760},{"sourceId":12487817,"sourceType":"datasetVersion","datasetId":7880205},{"sourceId":396531,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":325366,"modelId":346207},{"sourceId":456632,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":370292,"modelId":391177},{"sourceId":457537,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":371029,"modelId":391926},{"sourceId":474932,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":382117,"modelId":401673}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n---\n<h1 style=\"text-align: center; color: red;\"> 🏆SPARK 3rd BraTS-Africa Brain Hackaton</h1>\n<h2 style=\"text-align: center; color: black;\">  Brain Tumor Segmentation</h2>\n\n\n**<center> SPARK ACADEMY 2025 - DEEP LEARNING AND MEDICAL IMAGING  <center>**\n\n<hr>\n<hr>\n\n**<center>General Informations<center>** \n\n*<center> Autor :  French Team <center>*\n\n*<center>  Date : 13 May  2025  <center>*\n\n---","metadata":{}},{"cell_type":"markdown","source":"## 🎯 **Overview**\nIn this hackaton, you worked with **3D MRI scans** from the **[BraTS2021 dataset](https://www.kaggle.com/datasets/dschettler8845/brats-2021-task1)** available on **Kaggle** . \nThe goal of this competition is to tune SWIM UNETR models to make it more efficient for image segmentation\n","metadata":{}},{"cell_type":"code","source":"!pip -q install torchio monai medpy wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T14:09:52.933448Z","iopub.execute_input":"2025-09-17T14:09:52.933630Z","iopub.status.idle":"2025-09-17T14:11:31.147450Z","shell.execute_reply.started":"2025-09-17T14:09:52.933614Z","shell.execute_reply":"2025-09-17T14:11:31.146490Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tarfile\nimport os\n\n# Define file paths\ninput_tar_path = '/kaggle/input/brats-2021-task1/BraTS2021_Training_Data.tar'\noutput_dir = '/kaggle/working/extracted_data'\n\n# Create output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# Extract tar file\nwith tarfile.open(input_tar_path, 'r') as tar:\n    tar.extractall(path=output_dir)\n\nprint(f\"Extraction complete! Files are saved in: {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:07:20.820424Z","iopub.status.idle":"2025-07-16T15:07:20.820647Z","shell.execute_reply.started":"2025-07-16T15:07:20.820536Z","shell.execute_reply":"2025-07-16T15:07:20.820547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport wandb\nimport numpy as np\nimport nibabel as nib\nfrom glob import glob\nfrom tqdm import tqdm\nfrom statistics import mean\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nimport torchio as tio\nfrom torchio.data import SubjectsLoader, SubjectsDataset\n\nfrom monai.networks.nets import SwinUNETR\nfrom monai.losses import DiceLoss, DiceCELoss\n\ndevice  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.__version__, device\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:09:59.615028Z","iopub.execute_input":"2025-07-16T15:09:59.615708Z","iopub.status.idle":"2025-07-16T15:10:36.830553Z","shell.execute_reply.started":"2025-07-16T15:09:59.615674Z","shell.execute_reply":"2025-07-16T15:10:36.829973Z"}},"outputs":[{"name":"stderr","text":"<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-07-16 15:10:20.673504: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752678621.030075      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752678621.134715      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"('2.6.0+cu124', device(type='cuda'))"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torchio as tio\nimport os\nfrom glob import glob\nimport torch.nn.functional as F\n\n\nNUM_CLASSES = 4  # Update this based on your actual classes\n\nclass LazySegmentationDataset(Dataset):\n    def __init__(self, root_dir: str, transform=None, WT=False):\n        self.transform = transform\n        \n        # Just store file paths\n        self.flair_paths = sorted(glob(os.path.join(root_dir, \"**\", \"*t2f*.nii\"), recursive=True))\n        self.t1_paths = sorted(glob(os.path.join(root_dir, \"**\", \"*t1n.nii\"), recursive=True))\n        self.t1ce_paths = sorted(glob(os.path.join(root_dir, \"**\", \"*t1c*.nii\"), recursive=True))\n        self.t2_paths = sorted(glob(os.path.join(root_dir, \"**\", \"*t2w.nii\"), recursive=True))\n        self.mask_paths = sorted(glob(os.path.join(root_dir, \"**\", \"*seg*.nii\"), recursive=True))\n        \n        assert len(self.flair_paths) == len(self.t1_paths) == len(self.t1ce_paths) == len(self.t2_paths) == len(self.mask_paths), \\\n            \"Mismatch in number of images\"\n        \n        self.subject_paths = list(zip(self.flair_paths, self.t1_paths, self.t1ce_paths, self.t2_paths, self.mask_paths))\n    \n    def __len__(self):\n        return len(self.subject_paths)\n    \n\n\n    def __getitem__(self, idx):\n        flair_path, t1_path, t1ce_path, t2_path, mask_path = self.subject_paths[idx]\n    \n        subject = tio.Subject(\n            flair=tio.ScalarImage(flair_path),\n            t1=tio.ScalarImage(t1_path),\n            t1ce=tio.ScalarImage(t1ce_path),\n            t2=tio.ScalarImage(t2_path),\n            mask=tio.LabelMap(mask_path),\n        )\n    \n        if self.transform:\n            subject = self.transform(subject)\n    \n        # 1. Remap label 4 to 3\n        mask_tensor = subject['mask'][tio.DATA]  # [1, H, W, D]\n        mask_tensor[mask_tensor == 4] = 3\n        mask_tensor = mask_tensor.long()\n    \n        # 2. Remove channel dimension before one-hot encoding: [1, H, W, D] → [H, W, D]\n        mask_tensor = mask_tensor.squeeze(0)\n        if WT:\n            mask_tensor[mask_tensor >=1] = 1\n            NUM_CLASSES=2\n            # 3. One-hot encode: [H, W, D] → [C, H, W, D]\n            mask_onehot = F.one_hot(mask_tensor, num_classes=NUM_CLASSES).permute(3, 0, 1, 2).float()\n        else:\n            NUM_CLASSES=4\n            mask_onehot = F.one_hot(mask_tensor, num_classes=NUM_CLASSES).permute(3, 0, 1, 2).float()\n    \n        # 4. Replace mask with one-hot version\n        subject['mask'].set_data(mask_onehot)\n\n        return subject","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:10:50.652993Z","iopub.execute_input":"2025-07-16T15:10:50.654036Z","iopub.status.idle":"2025-07-16T15:10:50.663007Z","shell.execute_reply.started":"2025-07-16T15:10:50.654008Z","shell.execute_reply":"2025-07-16T15:10:50.662352Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"transforms = tio.Compose([\n    tio.ToCanonical(),\n    tio.ZNormalization(),\n    tio.Resample((2.4, 2.4, 2.2)),\n    tio.RescaleIntensity(out_min_max=(0, 1), percentiles=(0.05, 99.5)),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:10:54.039433Z","iopub.execute_input":"2025-07-16T15:10:54.039677Z","iopub.status.idle":"2025-07-16T15:10:54.044075Z","shell.execute_reply.started":"2025-07-16T15:10:54.039661Z","shell.execute_reply":"2025-07-16T15:10:54.043461Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nWT=False\n\nfull_dataset = LazySegmentationDataset('DATA PATH', transform=transforms,WT=WT)\n\n\n# Define split lengths\ntotal_size = len(full_dataset)\ntrain_size = int(0.9 * total_size)\nval_size = int(0.1 * total_size)\n# test_size = total_size - train_size - val_size\n\n# Optional: set manual seed for reproducibility\ngenerator = torch.Generator().manual_seed(42)\n\ntrain_dataset, val_dataset = random_split(\n    full_dataset, [train_size, val_size], generator=generator\n)\n\ntrain_loader = SubjectsLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\nval_loader   = SubjectsLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)\n# test_loader  = SubjectsLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:11:34.016955Z","iopub.execute_input":"2025-07-16T15:11:34.017727Z","iopub.status.idle":"2025-07-16T15:11:34.716317Z","shell.execute_reply.started":"2025-07-16T15:11:34.017703Z","shell.execute_reply":"2025-07-16T15:11:34.715600Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"len(train_loader), len(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:11:38.869511Z","iopub.execute_input":"2025-07-16T15:11:38.869773Z","iopub.status.idle":"2025-07-16T15:11:38.874779Z","shell.execute_reply.started":"2025-07-16T15:11:38.869754Z","shell.execute_reply":"2025-07-16T15:11:38.874099Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(27, 3)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModalitySEAttention(nn.Module):\n    def __init__(self, in_channels=4, reduction=2):\n        super(ModalitySEAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)  # Global average pooling\n        self.fc = nn.Sequential(\n            nn.Conv3d(in_channels, in_channels // reduction, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(in_channels // reduction, in_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # x: (B, 4, H, W, D)\n        attn = self.avg_pool(x)     # shape: (B, 4, 1, 1, 1)\n        attn = self.fc(attn)        # shape: (B, 4, 1, 1, 1)\n        return x * attn+x            # modality-wise scaling\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T20:46:43.727438Z","iopub.execute_input":"2025-07-15T20:46:43.727778Z","iopub.status.idle":"2025-07-15T20:46:43.733090Z","shell.execute_reply.started":"2025-07-15T20:46:43.727756Z","shell.execute_reply":"2025-07-15T20:46:43.732412Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class SwinUNetRMaskedAutoencoder1(nn.Module):\n    def __init__(self, img_size=96, in_channels=4, patch_size=2):\n        super().__init__()\n        self.encoder = SwinUNETR(\n            \n            in_channels=in_channels,\n            out_channels=4,  # Dummy output\n            feature_size=48,\n            use_checkpoint=False\n        )\n \n    def forward(self, x):\n        encoded = self.encoder(x)\n        # decoded = self.decoder(encoded)\n        return encoded\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:11:57.551493Z","iopub.execute_input":"2025-07-16T15:11:57.551755Z","iopub.status.idle":"2025-07-16T15:11:57.556275Z","shell.execute_reply.started":"2025-07-16T15:11:57.551736Z","shell.execute_reply":"2025-07-16T15:11:57.555534Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"Recon_model=SwinUNetRMaskedAutoencoder1(img_size=96, in_channels=4, patch_size=2)\nRecon_model = torch.nn.DataParallel(Recon_model, device_ids=[0, 1])  # Use GPU 0 and GPU 1\nRecon_model = Recon_model.to(device)\noriginal=SwinUNETR(\n            \n            in_channels=4,\n            out_channels=4,  \n            feature_size=48,\n            use_checkpoint=False\n        )\n# Recon_model.load_state_dict(torch.load(\"/kaggle/input/extended/pytorch/default/1/best_swinunetr_ssl_extended.pth\"))\n\n# Recon_model.module.encoder.out=original.out\nRecon_model.load_state_dict(torch.load(\"/kaggle/input/best_2021_pretrain/pytorch/default/1/best_swinunetr_extended_20_epochs.pth\"))\nRecon_model=Recon_model.to(device)\n# for name, param in Recon_model.module.named_parameters():\n#     if name.startswith(\"enc.\"):  # Only allow decoder head to train\n#         param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:13:38.933013Z","iopub.execute_input":"2025-07-16T15:13:38.933306Z","iopub.status.idle":"2025-07-16T15:13:40.369135Z","shell.execute_reply.started":"2025-07-16T15:13:38.933284Z","shell.execute_reply":"2025-07-16T15:13:40.368526Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from torchinfo import summary\nsummary(Recon_model, input_size=\n        (1, 4, 96, 96, 96))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T20:48:33.024983Z","iopub.execute_input":"2025-07-15T20:48:33.025913Z","iopub.status.idle":"2025-07-15T20:48:33.267629Z","shell.execute_reply.started":"2025-07-15T20:48:33.025888Z","shell.execute_reply":"2025-07-15T20:48:33.267025Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"===================================================================================================================\nLayer (type:depth-idx)                                            Output Shape              Param #\n===================================================================================================================\nDataParallel                                                      [1, 4, 96, 96, 96]        --\n├─SwinUNetRMaskedAutoencoder1: 1-1                                [1, 4, 96, 96, 96]        62,191,990\n├─SwinUNetRMaskedAutoencoder1: 1-2                                --                        --\n│    └─SwinUNETR: 2-1                                             [1, 4, 96, 96, 96]        62,191,990\n│    └─SwinUNETR: 2-2                                             --                        --\n│    │    └─SwinTransformer: 3-1                                  [1, 48, 48, 48, 48]       8,063,154\n│    │    └─SwinTransformer: 3-2                                  --                        8,063,154\n│    │    └─UnetrBasicBlock: 3-3                                  [1, 48, 96, 96, 96]       67,584\n│    │    └─UnetrBasicBlock: 3-4                                  --                        67,584\n│    │    └─UnetrBasicBlock: 3-5                                  [1, 48, 48, 48, 48]       124,416\n│    │    └─UnetrBasicBlock: 3-6                                  --                        124,416\n│    │    └─UnetrBasicBlock: 3-7                                  [1, 96, 24, 24, 24]       497,664\n│    │    └─UnetrBasicBlock: 3-8                                  --                        497,664\n│    │    └─UnetrBasicBlock: 3-9                                  [1, 192, 12, 12, 12]      1,990,656\n│    │    └─UnetrBasicBlock: 3-10                                 --                        1,990,656\n│    │    └─UnetrBasicBlock: 3-11                                 [1, 768, 3, 3, 3]         31,850,496\n│    │    └─UnetrBasicBlock: 3-12                                 --                        31,850,496\n│    │    └─UnetrUpBlock: 3-13                                    [1, 384, 6, 6, 6]         14,598,144\n│    │    └─UnetrUpBlock: 3-14                                    --                        14,598,144\n│    │    └─UnetrUpBlock: 3-15                                    [1, 192, 12, 12, 12]      3,649,536\n│    │    └─UnetrUpBlock: 3-16                                    --                        3,649,536\n│    │    └─UnetrUpBlock: 3-17                                    [1, 96, 24, 24, 24]       912,384\n│    │    └─UnetrUpBlock: 3-18                                    --                        912,384\n│    │    └─UnetrUpBlock: 3-19                                    [1, 48, 48, 48, 48]       228,096\n│    │    └─UnetrUpBlock: 3-20                                    --                        228,096\n│    │    └─UnetrUpBlock: 3-21                                    [1, 48, 96, 96, 96]       209,664\n│    │    └─UnetrUpBlock: 3-22                                    --                        209,664\n│    │    └─UnetOutBlock: 3-23                                    [1, 4, 96, 96, 96]        196\n│    │    └─UnetOutBlock: 3-24                                    --                        196\n===================================================================================================================\nTotal params: 374,909,138\nTrainable params: 374,909,138\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 317.90\n===================================================================================================================\nInput size (MB): 14.16\nForward/backward pass size (MB): 4181.81\nParams size (MB): 247.98\nEstimated Total Size (MB): 4443.95\n==================================================================================================================="},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom medpy.metric import binary as mpy\n\ndef compute_batch_metrics(logits, onehot_targets):\n    pred = torch.argmax(logits, dim=1).cpu().numpy()          # [B, H, W, D]\n    true = torch.argmax(onehot_targets, dim=1).cpu().numpy()  # [B, H, W, D]\n\n    metrics = {\n        'ET': {'dice': [], 'hd95': []},     # Label 3\n        'NETC': {'dice': [], 'hd95': []},   # Label 1\n        'SNTC': {'dice': [], 'hd95': []},   # Label 2\n        'TC': {'dice': [], 'hd95': []},     # Labels 1+3\n        'WT': {'dice': [], 'hd95': []},     # Labels 1+2+3\n    }\n\n    for b in range(pred.shape[0]):\n        p, t = pred[b], true[b]\n\n        masks = {\n            'ET':   ((p == 3).astype(np.uint8), (t == 3).astype(np.uint8)),\n            'NETC': ((p == 1).astype(np.uint8), (t == 1).astype(np.uint8)),\n            'SNTC': ((p == 2).astype(np.uint8), (t == 2).astype(np.uint8)),\n            'TC':   (((p == 1) | (p == 3)).astype(np.uint8), ((t == 1) | (t == 3)).astype(np.uint8)),\n            'WT':   (((p > 0).astype(np.uint8)), ((t > 0).astype(np.uint8))),\n        }\n\n        for region, (pb, tb) in masks.items():\n            intersection = (pb & tb).sum()\n            union = pb.sum() + tb.sum()\n            dice = 2 * intersection / union if union > 0 else 1.0 if pb.sum() == tb.sum() == 0 else 0.0\n\n            try:\n                hd95 = mpy.hd95(pb, tb) if pb.sum() and tb.sum() else 0.0\n            except:\n                hd95 = np.nan\n\n            metrics[region]['dice'].append(dice)\n            metrics[region]['hd95'].append(hd95)\n\n    # Aggregate\n    summary = []\n    for region, values in metrics.items():\n        summary.append({\n            'Region': region,\n            'Dice': round(np.nanmean(values['dice']), 4),\n            'Hausdorff95': round(np.nanmean(values['hd95']), 4)\n        })\n\n    return summary\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from monai.losses import DiceLoss, TverskyLoss\n\n# Initialize both losses\ndice_loss = DiceLoss(to_onehot_y=False, softmax=True)\ntversky_loss = TverskyLoss(to_onehot_y=False, softmax=True, alpha=0.3, beta=0.7)\nce_loss_fn = torch.nn.CrossEntropyLoss()\n\ndef combined_loss(logits, targets):\n    return 0.7 * dice_loss(logits, targets) + 0.2 * tversky_loss(logits, targets)+0.1*ce_loss_fn(logits, targets)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\ndef train_one_epoch_gard_acc(model, loader, optimizer):\n    model.train()\n    running_loss = 0.0\n    accum_steps = 4\n    optimizer.zero_grad()\n\n    for i, batch in enumerate(tqdm(train_loader)):\n        x = torch.cat([\n            batch['flair'][tio.DATA],\n            batch['t1'][tio.DATA],\n            batch['t1ce'][tio.DATA],\n            batch['t2'][tio.DATA],\n        ], dim=1).to(device)\n\n        y = batch['mask'][tio.DATA].to(device)  # One-hot encoded [B, C, H, W, D]\n\n        logits = model(x)\n        loss = combined_loss(logits, y)\n        loss = loss / accum_steps\n        loss.backward()\n    \n        if (i + 1) % accum_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n        running_loss += loss.item()\n\n    return running_loss / len(loader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\ndef train_one_epoch(model, loader, optimizer):\n    model.train()\n    running_loss = 0.0\n\n    for batch in tqdm(loader):\n        x = torch.cat([\n            batch['flair'][tio.DATA],\n            batch['t1'][tio.DATA],\n            batch['t1ce'][tio.DATA],\n            batch['t2'][tio.DATA],\n        ], dim=1).to(device)\n\n        y = batch['mask'][tio.DATA].to(device)  # One-hot encoded [B, C, H, W, D]\n\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = combined_loss(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    return running_loss / len(loader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef validate(model, loader):\n    model.eval()\n    val_loss = 0.0\n    all_logits = []\n    all_targets = []\n\n    for batch in tqdm(loader):\n        x = torch.cat([\n            batch['flair'][tio.DATA],\n            batch['t1'][tio.DATA],\n            batch['t1ce'][tio.DATA],\n            batch['t2'][tio.DATA],\n        ], dim=1).to(device)\n\n        y = batch['mask'][tio.DATA].to(device)\n\n        logits = model(x)\n        loss = combined_loss(logits, y)\n        val_loss += loss.item()\n\n        all_logits.append(logits.cpu())\n        all_targets.append(y.cpu())\n\n    avg_val_loss = val_loss / len(loader)\n\n    # Concatenate predictions and ground truth across batches\n    all_logits = torch.cat(all_logits, dim=0)\n    all_targets = torch.cat(all_targets, dim=0)\n\n    metrics = compute_batch_metrics(all_logits, all_targets)  # list of dicts\n\n    return avg_val_loss, metrics\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    avg_val_loss = val_loss / len(loader)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import wandb\n# wandb.login(key=\"\")\n\n# wandb.init(project=\"\", name=\"Swin_finetune_brats_v1\", config={\n#     \"epochs\": 100,\n#     \"batch_size\": 2,\n#     \"img_size\": (96, 96, 64),\n#     \"model\": \"SwinUNETR\",\n#     \"loss\": \"Dice + Tversky+CE\",\n# })\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(Recon_model.parameters(), lr=1e-5)\nnum_epochs = 100\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n# Parameters for warm restarts\nT_0 = 10          # Initial restart period\nT_mult = 2        # Factor to increase restart period (10 -> 20 -> 40...)\neta_min = 1e-7    # Minimum learning rate\n\nscheduler = CosineAnnealingWarmRestarts(\n    optimizer, \n    T_0=T_0, \n    T_mult=T_mult, \n    eta_min=eta_min\n)\n\nbest_val_loss = float('inf')\n\nfor epoch in range(1, num_epochs + 1):\n    train_loss = train_one_epoch_gard_acc(Recon_model, train_loader, optimizer)\n    val_loss, val_metrics = validate(Recon_model, val_loader)\n    \n    scheduler.step()\n    current_lr = scheduler.get_last_lr()[0]\n    \n    print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\")\n    \n    wandb.log({\n        \"epoch\": epoch,\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss,\n        \"learning_rate\": current_lr\n    })\n    \n    for metric in val_metrics:\n        region = metric['Region']\n        wandb.log({\n            f\"{region}/dice\": metric['Dice'],\n            f\"{region}/hd95\": metric['Hausdorff95'],\n        })\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': Recon_model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_val_loss': best_val_loss,\n            'val_metrics': val_metrics\n        }, \"best_swinunetr_warm_restart_v0.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}